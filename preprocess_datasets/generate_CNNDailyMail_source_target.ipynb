{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from math import log, sqrt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from priberam_summarizer.extractive_summarization_decoder import ExtractiveCoverageSummarizationDecoder\n",
    "from priberam_summarizer.parts import SentencePart, ConceptPart\n",
    "from priberam_summarizer.document import Document\n",
    "from priberam_summarizer.sentence import Sentence\n",
    "from priberam_summarizer.token import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_CNN_Dailymail(data_path):    \n",
    "    corpus = []\n",
    "    files = os.listdir(data_path)\n",
    "    for index, doc_filename in enumerate(files):\n",
    "        text = {}\n",
    "        field = 'UNK'\n",
    "        with open(os.path.join(data_path, doc_filename), 'r', encoding='utf8') as fp:\n",
    "            for line in fp:\n",
    "                line = line.strip()                \n",
    "                if line.startswith('[SN]'):\n",
    "                    field = line[4:-4]\n",
    "                elif len(line) != 0:\n",
    "                    text[field] = text.get(field, []) + [line]\n",
    "\n",
    "            if 'HighlightsOrg' not in text or 'StoryOrg' not in text:\n",
    "                # print('Not found highlights or story for {}'.format(doc_filename))\n",
    "                continue\n",
    "\n",
    "            doc = Document()\n",
    "            for sentence in nltk.sent_tokenize('\\n'.join(text['StoryOrg'])):\n",
    "                sent = Sentence(sentence)\n",
    "                for token in nltk.word_tokenize(sentence):\n",
    "                    sent.tokens.append(Token(token))\n",
    "                doc.body.append(sent)\n",
    "\n",
    "            summary = Document()\n",
    "            for sentence in text['HighlightsOrg']:\n",
    "                sent = Sentence(sentence)\n",
    "                for token in nltk.word_tokenize(sentence):\n",
    "                    sent.tokens.append(Token(token))\n",
    "                summary.body.append(sent)\n",
    "            corpus.append((doc, summary))\n",
    "            \n",
    "        if index % 1000 == 0:\n",
    "            print('Processed {} / {}'.format(index, len(files)))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 / 1090\n",
      "Processed 1000 / 1090\n"
     ]
    }
   ],
   "source": [
    "corpus = read_CNN_Dailymail('/home/ppb/Data/summarization/orignals/CNN_Dailymail/cnn/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_tfs(corpus):\n",
    "    # Compute TF frequencies for unigrams and bigrams\n",
    "    for doc, summary in corpus:\n",
    "        for document in [doc, summary]:            \n",
    "            setattr(document, 'bigrams', Counter())\n",
    "            for sentence in document.sentences + summary.sentences:                \n",
    "                setattr(sentence, 'bigrams', Counter())\n",
    "                tokens = [token.word for token in sentence.tokens]\n",
    "                tokens = [token.lower() for token in tokens]                \n",
    "                tokens = ['__start__'] + tokens + ['__end__']\n",
    "                sentence.bigrams = Counter(zip(tokens, tokens[1:]))\n",
    "                # update the count in document                \n",
    "                document.bigrams.update(sentence.bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_tfs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_bigrams_idfs(corpus):    \n",
    "\n",
    "    # Compute IDFs. Number of docs / number of documents the term happen    \n",
    "    bigram_idfs = Counter()    \n",
    "    for doc, summary in corpus:\n",
    "        for document in [doc, summary]:            \n",
    "            bigram_idfs.update({token: 1 for token in document.bigrams.keys()})\n",
    "\n",
    "    num_docs = len(corpus) * 2    \n",
    "\n",
    "    # inverse document frequency smooth    \n",
    "    bigram_idfs = {key: log(num_docs / (1 + value)) for key, value in bigram_idfs.items()}\n",
    "    bigram_idfs['__oov__'] = log(float(num_docs), 10)\n",
    "    \n",
    "    return bigram_idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_idfs = compute_bigrams_idfs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_tf_idfs(corpus, bigram_idfs):\n",
    "    for doc, summary in corpus:\n",
    "        for document in [doc, summary]:                \n",
    "            for sentence in document.sentences:                \n",
    "                setattr(sentence, 'bigram_tfidf', Counter())                \n",
    "                # compute tf_idf for bigrams\n",
    "                sentence.bigram_tfidf = {key: freq * bigram_idfs[key] for key, freq in sentence.bigrams.items()}\n",
    "                norm_factor = sqrt(sum([value * value for value in sentence.bigram_tfidf.values()]))\n",
    "                sentence.bigram_tfidf = {key: value / norm_factor if norm_factor > 0 else value for key, value in sentence.bigram_tfidf.items()}                                \n",
    "                delattr(sentence, 'bigrams')            \n",
    "            delattr(document, 'bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_tf_idfs(corpus, bigram_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('__start__', 'a'): 0.10168304590465235,\n",
       " ('a', 'magnitude-7.8'): 0.421108005423122,\n",
       " ('earthquake', 'struck'): 0.35602536926803,\n",
       " ('magnitude-7.8', 'earthquake'): 0.40267439041025765,\n",
       " ('nepal', 'on'): 0.39494632164604654,\n",
       " ('on', 'saturday'): 0.2532715187541065,\n",
       " ('saturday', '__end__'): 0.37570221426466976,\n",
       " ('struck', 'nepal'): 0.40267439041025765}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][1].sentences[0].bigram_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_oracle(bigram_idfs, document, highlight):    \n",
    "    parts = []\n",
    "\n",
    "    # compute unigram and bigram count for the summary\n",
    "    bigrams = Counter()\n",
    "\n",
    "    tokens = [token.word for token in highlight.tokens]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = ['__start__'] + tokens + ['__end__']\n",
    "    bigrams = Counter(zip(tokens, tokens[1:]))\n",
    "\n",
    "    # compute tf_idf for bigrams\n",
    "    bigram_tfidf = {key: freq * bigram_idfs.get(key, bigram_idfs['__oov__']) for key, freq in bigrams.items()}\n",
    "    norm_factor = sqrt(sum([value * value for value in bigram_tfidf.values()]))\n",
    "    bigram_tfidf = {key: value / norm_factor if norm_factor > 0 else value for key, value in bigram_tfidf.items()}\n",
    "\n",
    "    sentence_scores = []\n",
    "    concept_sentences = {}\n",
    "    for sentence_index, sentence in enumerate(document.sentences):\n",
    "        part = SentencePart(sentence)\n",
    "        sentence_score = 0\n",
    "        for concept in set(bigram_tfidf).intersection(set(sentence.bigram_tfidf)):\n",
    "            concept_sentences[concept] = concept_sentences.get(concept, []) + [sentence_index]\n",
    "            sentence_score += bigram_tfidf[concept]\n",
    "        sentence_scores.append(sentence_score)\n",
    "        part.active = True\n",
    "        part.value = sentence_score\n",
    "        parts.append(part)\n",
    "\n",
    "    # Concept scores are based on summary tf-idf concepts\n",
    "    for concept in sorted(bigram_tfidf):\n",
    "        part = ConceptPart(concept, bigram_tfidf[concept], concept_sentences.get(concept, []))\n",
    "        parts.append(part)\n",
    "\n",
    "    scores = [part.value for part in parts]\n",
    "    decoder_aux = ExtractiveCoverageSummarizationDecoder()\n",
    "    decoder_aux.max_words = 30\n",
    "    selected_sentences, _, predicted_concepts = decoder_aux.summarize_coverage(parts, scores)\n",
    "\n",
    "    # Final summary and its score.\n",
    "    summary = Document()\n",
    "    summary.name = 'Oracle: '\n",
    "    for selected_sentence in selected_sentences:\n",
    "        summary.body.append(document.sentences[selected_sentence])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 / 1090\n",
      "Processed 1000 / 1090\n"
     ]
    }
   ],
   "source": [
    "f_source = open('source.txt', 'w', encoding='utf8')\n",
    "f_target = open('target.txt', 'w', encoding='utf8')\n",
    "for index, (document, summary) in enumerate(corpus):\n",
    "    for sentence in summary.sentences:\n",
    "        oracle = extract_oracle(bigram_idfs, document, sentence)        \n",
    "        bigram_tfidf = Counter()\n",
    "        for s in oracle.sentences:\n",
    "            bigram_tfidf.update(s.bigram_tfidf)\n",
    "        score = sum([bigram_tfidf[k] for k in set(bigram_tfidf).intersection(set(sentence.bigram_tfidf))]) / len(sentence.bigram_tfidf)\n",
    "\n",
    "        if score >= 0.05:\n",
    "            f_source.write(oracle.get_text().replace('\\n', ' ') + '\\n')\n",
    "            f_target.write(sentence.text.replace('\\n', ' ') + '\\n')            \n",
    "\n",
    "    if index % 1000 == 0:\n",
    "        print('Processed {} / {}'.format(index, len(corpus)))\n",
    "f_source.close()\n",
    "f_target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
